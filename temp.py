import numpy as np
import os
import argparse
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from collections import defaultdict
from sklearn.cluster import SpectralClustering
from sklearn.metrics import silhouette_score
import json
import seaborn as sns
import matplotlib.patches as patches
import networkx as nx


parser = argparse.ArgumentParser()
parser.add_argument('--model_name', type=str, default="OLMoE", help='Name of the model')
parser.add_argument('--input_name', type=str, default="sonnet", help='Name of the dataset')
parser.add_argument('--phrase_mode', type=str, default="decode", help='encode or decode')
parser.add_argument('--top_k', type=int, default=8,help='Value of top_k')
parser.add_argument('--num_of_experts_pre_layer', type=int, default=64,help='Number of experts per layer')
args = parser.parse_args()


def affinity_intra_layer(routing_data, num_of_experts_pre_layer):
    num_tokens, num_layers, top_k = routing_data.shape
    affinity_matrix_all_layers = np.zeros((num_layers, num_of_experts_pre_layer, num_of_experts_pre_layer))

    for layer in range(num_layers):
        for token in range(num_tokens):
            experts_intra_layer = routing_data[token,layer]
            for i in range(top_k):
                for j in range (i+1, top_k):
                    expert_i, expert_j = experts_intra_layer[i], experts_intra_layer[j]
                    affinity_matrix_all_layers[layer, expert_i, expert_j] += 1
                    affinity_matrix_all_layers[layer, expert_j, expert_i] += 1 # å¯¹ç§°çŸ©é˜µ
    return affinity_matrix_all_layers


# def cluster_experts_per_layer(affinity_matrix_all_layers):
#     num_layers, num_experts, _ = affinity_matrix_all_layers.shape
    
#     placement = []
#     for co_mat in co_matrices:
#         # é™ç»´åèšç±»
#         pca = PCA(n_components=10)
#         reduced = pca.fit_transform(co_mat)
#         kmeans = KMeans(n_clusters=num_gpus, random_state=0).fit(reduced)
#         groups = defaultdict(list)
#         for expert_id, cluster_id in enumerate(kmeans.labels_):
#             groups[cluster_id].append(expert_id)
#         placement.append(groups)
#     return placement  # List[Dict[int, List[int]]]

# def cluster_and_plot(co_matrix, n_clusters=4, use_pca=False, pca_dims=10, title=''):
#     # æ¯ä¸ªä¸“å®¶çš„å…±ç°å‘é‡æ˜¯å…±ç°çŸ©é˜µçš„ä¸€è¡Œ
#     features = co_matrix.astype(np.float32)
    
#     if use_pca:
#         pca = PCA(n_components=pca_dims)
#         reduced = pca.fit_transform(features)
#         features_used = reduced
#     else:
#         features_used = features

#     # èšç±»
#     kmeans = KMeans(n_clusters=n_clusters, random_state=42)
#     labels = kmeans.fit_predict(features_used)

#     # 2D å¯è§†åŒ–ï¼ˆé™åˆ°2ç»´ç”»å›¾ï¼‰
#     vis_features = PCA(n_components=2).fit_transform(features_used)

#     plt.figure(figsize=(5, 5))
#     plt.title(title)
#     plt.scatter(vis_features[:, 0], vis_features[:, 1], c=labels, cmap='tab10', s=60)
#     for i in range(len(labels)):
#         plt.text(vis_features[i, 0], vis_features[i, 1], str(i), fontsize=8, ha='center', va='center')
#     plt.xlabel("PCA1")
#     plt.ylabel("PCA2")
#     plt.grid(True)
#     #plt.show()
#     plt.savefig(title, dpi=300, bbox_inches="tight")
#     plt.close()

#     return labels

# def compare_cluster_labels(labels1, labels2, num_groups=4, verbose=True):
#     assert len(labels1) == len(labels2), "æ ‡ç­¾é•¿åº¦ä¸ä¸€è‡´"
#     num_items = len(labels1)

#     # è®°å½•ä¸ä¸€è‡´çš„ä¸“å®¶ç¼–å·
#     disagreement = []
#     for i in range(num_items):
#         if labels1[i] != labels2[i]:
#             disagreement.append(i)

#     # è®¡ç®—ä¸€è‡´ç‡
#     agreement_rate = 1 - len(disagreement) / num_items

#     if verbose:
#         print(f"æ€»ä¸“å®¶æ•°: {num_items}")
#         print(f"åˆ†ç»„ä¸ä¸€è‡´çš„ä¸“å®¶æ•°: {len(disagreement)}")
#         print(f"ä¸€è‡´ç‡: {agreement_rate:.2%}")
#         if disagreement:
#             print("ä¸åŒåˆ†ç»„çš„ä¸“å®¶ç¼–å·:", disagreement)

#     return {
#         "total": num_items,
#         "diff_count": len(disagreement),
#         "agreement_rate": agreement_rate,
#         "disagreement_indices": disagreement
#     }



# def cluster_experts_per_layer(co_matrix_all_layers, num_groups=4):
#     """
#     :param co_matrix_all_layers: shape (num_layers, 64, 64)
#     :return: List[Dict[group_id -> List[expert_ids]]]
#     """
#     num_layers, num_experts, _ = co_matrix_all_layers.shape
#     placement = []

#     for layer in range(num_layers):
#         co_matrix = co_matrix_all_layers[layer]

#         # å°†æ¯ä¸ªä¸“å®¶çš„å…±ç°å‘é‡ä½œä¸ºç‰¹å¾ï¼ˆ64ç»´ï¼‰
#         features = co_matrix.astype(np.float32)

#         # å¯é€‰é™ç»´ï¼ˆæœ‰æ—¶èƒ½è®©èšç±»æ›´ç¨³å®šï¼‰
#         pca = PCA(n_components=10)
#         reduced = pca.fit_transform(features)

#         # KMeans èšç±»
#         kmeans = KMeans(n_clusters=num_groups, random_state=0)
#         labels = kmeans.fit_predict(reduced)

#         # æ„å»º group -> expert_id çš„æ˜ å°„
#         group_dict = {i: [] for i in range(num_groups)}
#         for expert_id, group_id in enumerate(labels):
#             group_dict[group_id].append(expert_id)

#         placement.append(group_dict)

#     return placement  # List[Dict[group_id -> List[expert_ids]]]

# def spectral_cluster_and_visualize(co_matrix, num_groups=4, layer_id=0, save_path=None):
#     """
#     å¯¹æŸå±‚å…±ç°çŸ©é˜µåšè°±èšç±»å¹¶ä¿å­˜å¯è§†åŒ–å›¾åƒ

#     :param co_matrix: shape (64, 64), å…±ç°çŸ©é˜µ
#     :param num_groups: èšç±»ç»„æ•°ï¼ˆé€šå¸¸ç­‰äºGPUæ•°ï¼‰
#     :param layer_id: å½“å‰å±‚ç¼–å·ï¼Œç”¨äºå‘½åä¿å­˜æ–‡ä»¶
#     :param save_path: å¯é€‰ï¼Œä¿å­˜å›¾ç‰‡çš„è·¯å¾„ï¼Œé»˜è®¤ä¿å­˜ä¸º spectral_clustering_layer{layer_id}.png
#     :return: èšç±»æ ‡ç­¾ï¼Œshape (64,)
#     """
#     co_matrix = co_matrix.astype(np.float32)

#     # ---------------- Spectral Clustering ----------------
#     clustering = SpectralClustering(
#         n_clusters=num_groups,
#         affinity='precomputed',   # ç›´æ¥ä½¿ç”¨ä½ æä¾›çš„äº²å’ŒçŸ©é˜µ
#         assign_labels='kmeans',   # æˆ– 'discretize' ä¹Ÿå¯ä»¥è¯•è¯•
#         random_state=42
#     )
#     labels = clustering.fit_predict(co_matrix)

#     # ---------------- Visualization ----------------
#     # ç”¨ PCA é™åˆ°äºŒç»´ï¼Œåšå¯è§†åŒ–
#     pca = PCA(n_components=2)
#     reduced = pca.fit_transform(co_matrix)

#     plt.figure(figsize=(6, 6))
#     plt.title(f"Spectral Clustering - Layer {layer_id}")
#     plt.scatter(reduced[:, 0], reduced[:, 1], c=labels, cmap='tab10', s=60)

#     # æ ‡å‡ºæ¯ä¸ªç‚¹çš„ä¸“å®¶ç¼–å·
#     for i in range(len(labels)):
#         plt.text(reduced[i, 0], reduced[i, 1], str(i), fontsize=8, ha='center', va='center')

#     plt.xlabel("PCA1")
#     plt.ylabel("PCA2")
#     plt.grid(True)

#     filename = os.path.join(save_path, f"spectral_clustering_layer{layer_id}.png")

    
#     plt.savefig(filename)
#     plt.close()
#     print(f"å›¾åƒå·²ä¿å­˜åˆ°: {filename}")

#     return labels


# def cluster_experts_per_layer(affinity_matrix, num_cluster, layer_id, save_path):
#     # è°±èšç±»
#     model = SpectralClustering(
#         n_clusters=num_cluster,
#         affinity='precomputed',
#         random_state=42,
#         assign_labels='discretize'
#     )
    
#     labels = model.fit_predict(affinity_matrix)
    
#     silhouette = silhouette_score(affinity_matrix, labels, metric='precomputed') #è´¨é‡è¯„ä¼°
#     print(f"Layer {layer_id} - Silhouette Score: {silhouette:.3f}")
    
#     # ä¿å­˜èšç±»ç»“æœåˆ°JSONæ–‡ä»¶
#     cluster_dict = defaultdict(list)
#     for expert_id, cluster_id in enumerate(labels):
#         cluster_dict[int(cluster_id)].append(int(expert_id))  
    
#     result_dir = os.path.join(save_path, "cluster_results")
#     os.makedirs(result_dir, exist_ok=True)
#     with open(os.path.join(result_dir, f"layer_{layer_id}_clusters.json"), 'w') as f:
#         json.dump(cluster_dict, f, indent=2)
    

#     # å¯è§†åŒ–å¹¶ä¿å­˜å›¾ç‰‡
#     plt.figure(figsize=(12, 10))
#     # æ ¹æ®èšç±»æ ‡ç­¾é‡æ–°æ’åºçŸ©é˜µ
#     sorted_indices = np.argsort(labels)
#     sorted_matrix = affinity_matrix[sorted_indices][:, sorted_indices]
    
#     # ç»˜åˆ¶çƒ­åŠ›å›¾
#     ax = sns.heatmap(
#         sorted_matrix,
#         cmap="YlGnBu",
#         square=True,
#         cbar_kws={"shrink": 0.8},
#         xticklabels=sorted_indices,
#         yticklabels=sorted_indices
#     )
    
#     # æ·»åŠ åˆ†éš”çº¿
#     unique_labels = np.unique(labels)
#     current_pos = 0
#     for label in unique_labels:
#         count = np.sum(labels == label)
#         ax.axhline(current_pos + count, color='red', linewidth=2)
#         ax.axvline(current_pos + count, color='red', linewidth=2)
#         current_pos += count
    
#     plt.title(f"Layer {layer_id} Expert Affinity (Clustered)\nSilhouette Score: {silhouette:.3f}")
#     plt.xlabel("Expert ID")
#     plt.ylabel("Expert ID")
#     plt.tight_layout()
    
#     # ä¿å­˜å›¾ç‰‡
#     img_path = os.path.join(save_path, f"layer_{layer_id}_clusters.png")
#     plt.savefig(img_path, dpi=150, bbox_inches='tight')
#     plt.close()
    
#     return labels

def spectral_clustering_all_layers(affinity_matrix_all_layers, num_groups=4, save_json_path=None):
    num_layers = affinity_matrix_all_layers.shape[0]
    all_layer_groupings = {}

    for layer_id in range(num_layers):
        affinity_matrix = affinity_matrix_all_layers[layer_id].astype(np.float32)

        # è°±èšç±»ï¼ˆç›´æ¥ç”¨äº²å’ŒçŸ©é˜µï¼‰
        clustering = SpectralClustering(
            n_clusters=num_groups,
            affinity='precomputed',
            assign_labels='kmeans',
            random_state=42
        )
        labels = clustering.fit_predict(affinity_matrix)

        # æ„å»º group_id -> list of expert_id çš„å­—å…¸
        layer_grouping = defaultdict(list)
        for expert_id, group_id in enumerate(labels):
            layer_grouping[int(group_id)].append(int(expert_id))  # è½¬ä¸º int æ–¹ä¾¿å­˜å‚¨

        all_layer_groupings[f"layer_{layer_id}"] = layer_grouping

    # ä¿å­˜ä¸º JSON
    if save_json_path:
        os.makedirs(os.path.dirname(save_json_path), exist_ok=True)
        with open(save_json_path, 'w') as f:
            json.dump(all_layer_groupings, f, indent=4)
        print(f"èšç±»ç»“æœå·²ä¿å­˜åˆ°: {save_json_path}")

    return all_layer_groupings

def draw_spectral_cluster_map(affinity_matrix, labels, layer_id, save_dir):
    pca = PCA(n_components=2)
    reduced = pca.fit_transform(affinity_matrix)

    plt.figure(figsize=(6, 6))
    plt.title(f"Spectral Clustering - Layer {layer_id}")
    plt.scatter(reduced[:, 0], reduced[:, 1], c=labels, cmap='tab10', s=60)

    for i in range(len(labels)):
        plt.text(reduced[i, 0], reduced[i, 1], str(i), fontsize=8, ha='center', va='center')

    plt.xlabel("PCA1")
    plt.ylabel("PCA2")
    plt.grid(True)

    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, f"layer{layer_id}_spectral.png")
    plt.savefig(save_path)
    plt.close()
    print(f"å›¾åƒå·²ä¿å­˜: {save_path}")

def visualize_reordered_affinity(affinity_matrix, labels, layer_id, save_dir):
    """
    æ ¹æ®èšç±»æ ‡ç­¾å¯¹äº²å’ŒçŸ©é˜µè¿›è¡Œæ’åºï¼Œå¹¶ç”»å‡ºçƒ­åŠ›å›¾
    """
    os.makedirs(save_dir, exist_ok=True)

    # å°†ä¸“å®¶æŒ‰ç…§èšç±»é¡ºåºé‡æ–°æ’åº
    sorted_indices = np.argsort(labels)
    reordered_matrix = affinity_matrix[sorted_indices][:, sorted_indices]

    # ç”¨äºç”»çº¢çº¿çš„åˆ†ç•Œç‚¹
    group_sizes = [np.sum(labels == g) for g in sorted(set(labels))]
    boundaries = np.cumsum(group_sizes)

    plt.figure(figsize=(10, 9))
    ax = sns.heatmap(reordered_matrix, cmap='YlGnBu', square=True, cbar=True)

    # ç”»çº¢è‰²åˆ†ç•Œçº¿ï¼ˆè·³è¿‡æœ€åä¸€ä¸ªï¼Œå› ä¸ºé‚£æ˜¯è¾¹ç•Œå¤–ï¼‰
    for b in boundaries[:-1]:
        ax.axhline(b, color='red', linestyle='--', linewidth=1.2)
        ax.axvline(b, color='red', linestyle='--', linewidth=1.2)

    plt.title(f"Experts Affinity Matrix (Cluster Ordered) â€” Layer {layer_id}")
    plt.xlabel("Expert ID (cluster sorted)")
    plt.ylabel("Expert ID (cluster sorted)")

    save_path = os.path.join(save_dir, f"layer{layer_id}_affinity_clustered.png")
    plt.savefig(save_path, bbox_inches='tight')
    plt.close()
    print(f" èšç±»æ’åºçš„äº²å’Œå›¾å·²ä¿å­˜: {save_path}")

def visualize_affinity_with_clustered_ids(affinity_matrix, labels, layer_id, save_dir):
    """
    å¯¹äº²å’ŒçŸ©é˜µè¿›è¡Œèšç±»æ’åºï¼Œå¹¶æ˜¾ç¤ºæ’åºåçš„åŸå§‹ä¸“å®¶IDï¼ŒåŒæ—¶ç”»å‡ºèšç±»è¾¹ç•Œçº¿
    """
    os.makedirs(save_dir, exist_ok=True)

    num_experts = len(labels)

    # â‘  å¯¹ä¸“å®¶æŒ‰ç…§èšç±»é¡ºåºæ’åºï¼ˆå…ˆæŒ‰ group_idï¼Œå†æŒ‰ expert_id æ’å†…éƒ¨ï¼‰
    sorted_indices = np.argsort(labels)
    sorted_labels = labels[sorted_indices]
    sorted_expert_ids = [str(i) for i in sorted_indices]

    # â‘¡ é‡æ’äº²å’ŒçŸ©é˜µ
    reordered_matrix = affinity_matrix[sorted_indices][:, sorted_indices]

    # â‘¢ è®¡ç®—çº¢çº¿åˆ†ç»„è¾¹ç•Œ
    group_counts = [np.sum(sorted_labels == g) for g in sorted(set(labels))]
    boundaries = np.cumsum(group_counts)

    # â‘£ ç»˜åˆ¶çƒ­åŠ›å›¾
    plt.figure(figsize=(12, 10))
    ax = sns.heatmap(reordered_matrix, cmap='YlGnBu', square=True, cbar=True)

    # è®¾ç½® tick æ˜¾ç¤ºä¸º æ’åºåçš„åŸå§‹ä¸“å®¶ ID
    ax.set_xticks(np.arange(num_experts) + 0.5)
    ax.set_yticks(np.arange(num_experts) + 0.5)
    ax.set_xticklabels(sorted_expert_ids, rotation=90, fontsize=7)
    ax.set_yticklabels(sorted_expert_ids, rotation=0, fontsize=7)

    # â‘¤ ç”»çº¢è‰²è™šçº¿
    for b in boundaries[:-1]:
        ax.axhline(b, color='red', linestyle='--', linewidth=1.2)
        ax.axvline(b, color='red', linestyle='--', linewidth=1.2)

    plt.title(f"Affinity Matrix (Cluster Sorted + Expert IDs) â€” Layer {layer_id}")
    plt.xlabel("Expert ID (original, cluster sorted)")
    plt.ylabel("Expert ID (original, cluster sorted)")

    save_path = os.path.join(save_dir, f"layer{layer_id}_affinity_cluster_sorted_ids.png")
    plt.savefig(save_path, bbox_inches='tight')
    plt.close()
    print(f"èšç±»ä¸“å®¶ç¼–å·å›¾å·²ä¿å­˜: {save_path}")


def plot_expert_affinity_graph(affinity_matrix, layer_id, save_dir, threshold=5000):
    """
    å°†ä¸“å®¶äº²å’Œæ€§çŸ©é˜µè½¬æ¢ä¸ºå›¾ç»“æ„å¹¶å¯è§†åŒ–
    :param affinity_matrix: shape (64, 64), å…±ç°çŸ©é˜µ
    :param layer_id: å½“å‰å±‚ç¼–å·
    :param save_dir: å›¾ç‰‡ä¿å­˜ç›®å½•
    :param threshold: è¾¹æƒé˜ˆå€¼ï¼Œå°äºæ­¤å€¼ä¸ç”»ï¼ˆé˜²æ­¢å¤ªå¯†ï¼‰
    """
    os.makedirs(save_dir, exist_ok=True)

    G = nx.Graph()
    num_experts = affinity_matrix.shape[0]

    # æ·»åŠ èŠ‚ç‚¹
    for i in range(num_experts):
        G.add_node(i)

    # æ·»åŠ è¾¹ï¼ˆä»…ä¿ç•™é«˜äºé˜ˆå€¼çš„ï¼‰
    for i in range(num_experts):
        for j in range(i + 1, num_experts):
            weight = affinity_matrix[i, j]
            if weight > threshold:
                G.add_edge(i, j, weight=weight)

    # ç”Ÿæˆå¸ƒå±€ï¼ˆspring å¸ƒå±€æ¨¡æ‹Ÿç‰©ç†æ–¥åŠ›æ•ˆæœï¼‰
    pos = nx.spring_layout(G, seed=42)

    # æå–è¾¹æƒé‡ç”¨äºè®¾ç½®çº¿å®½
    edges = G.edges(data=True)
    edge_weights = [d['weight'] for _, _, d in edges]
    edge_colors = [min(w, 10000) for w in edge_weights]  # è£å‰ªæç«¯å€¼

    # ç»˜å›¾
    plt.figure(figsize=(10, 10))
    nx.draw_networkx_nodes(G, pos, node_size=400, node_color='skyblue')
    nx.draw_networkx_labels(G, pos, font_size=8)

    # è¾¹å®½åº¦å’Œé¢œè‰²æ˜ å°„æƒé‡
    nx.draw_networkx_edges(
        G, pos,
        edgelist=edges,
        width=[w / 5000 for w in edge_weights],  # ç¼©æ”¾ç²—ç»†
        edge_color=edge_colors,
        edge_cmap=plt.cm.Blues
    )

    plt.title(f"Expert Affinity Graph â€” Layer {layer_id} (threshold>{threshold})")
    plt.axis('off')

    save_path = os.path.join(save_dir, f"layer{layer_id}_affinity_graph.png")
    plt.savefig(save_path, bbox_inches='tight')
    plt.close()
    print(f"ğŸŒ ä¸“å®¶å›¾å·²ä¿å­˜: {save_path}")


def plot_expert_subgraph(affinity_matrix, center_expert_id, layer_id, save_dir, threshold=1000):
    """
    åªç”»æŸä¸ªä¸­å¿ƒä¸“å®¶çš„å­å›¾ï¼ˆå®ƒè‡ªå·± + æ‰€æœ‰è¿æ¥çš„ä¸“å®¶ï¼‰
    """
    os.makedirs(save_dir, exist_ok=True)
    num_experts = affinity_matrix.shape[0]

    G = nx.Graph()

    # æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹ï¼ˆå¯é€‰ï¼šä¹Ÿå¯ä»¥åªæ·»åŠ æœ‰è¿æ¥çš„ï¼‰
    for i in range(num_experts):
        G.add_node(i)

    # æ·»åŠ æ»¡è¶³é˜ˆå€¼çš„è¾¹
    for i in range(num_experts):
        for j in range(i + 1, num_experts):
            weight = affinity_matrix[i, j]
            if weight > threshold:
                G.add_edge(i, j, weight=weight)

    # æ£€æŸ¥ä¸­å¿ƒä¸“å®¶æ˜¯å¦å­˜åœ¨è¿æ¥
    if center_expert_id not in G:
        print(f"âš ï¸ Expert {center_expert_id} has no edges above threshold {threshold}")
        return

    # æå–å­å›¾ï¼šä¸­å¿ƒä¸“å®¶ + å®ƒè¿æ¥çš„èŠ‚ç‚¹
    neighbors = list(G.neighbors(center_expert_id))
    subgraph_nodes = neighbors + [center_expert_id]
    subgraph = G.subgraph(subgraph_nodes)

    # å¸ƒå±€ï¼šspring layout æ›´ç¾è§‚
    pos = nx.spring_layout(subgraph, seed=42)

    edge_weights = [d['weight'] for _, _, d in subgraph.edges(data=True)]

    # ç»˜å›¾
    plt.figure(figsize=(8, 8))
    nx.draw_networkx_nodes(subgraph, pos, node_size=600, node_color='skyblue')
    nx.draw_networkx_labels(subgraph, pos, font_size=9)
    nx.draw_networkx_edges(
        subgraph, pos,
        width=[w / 500 for w in edge_weights],
        edge_color=edge_weights,
        edge_cmap=plt.cm.Blues
    )

    plt.title(f"Expert Subgraph â€” Center: {center_expert_id} (Layer {layer_id})")
    plt.axis('off')

    save_path = os.path.join(save_dir, f"layer{layer_id}_subgraph_expert{center_expert_id}.png")
    plt.savefig(save_path, bbox_inches='tight')
    plt.close()
    print(f"ğŸ” ä¸“å®¶å­å›¾å·²ä¿å­˜: {save_path}")


if __name__ == "__main__":

    # model_name = "OLMoE"#Switch_Transformer OLMoE
    # input_name = "sonnet"
    # phrase_mode = "decode" #decode
    # top_k = 8 # ST:1,OL:8
    # num_of_experts_pre_layer = 64

    # fig_dir = f"cluster/spectral/test"
    # os.makedirs(fig_dir, exist_ok=True)

    model_name = args.model_name
    input_name = args.input_name
    phrase_mode = args.phrase_mode
    top_k = args.top_k
    num_of_experts_pre_layer = args.num_of_experts_pre_layer
    
    routing_data = np.load(f'expert_trace/{model_name}/{input_name}/top{top_k}/{phrase_mode}_routing_trace_1024.npy')
    
    affinity_matrix_all_layers = affinity_intra_layer(routing_data, num_of_experts_pre_layer)
    
    # for i in range(len(affinity_matirx_all_layers)):
    #     # test_matrix = affinity_matirx_all_layers[i]

    #     # labels_pca = cluster_and_plot(test_matrix, use_pca=True, pca_dims=10, title=f"layer{i}_With PCA (10 dims)")
    #     # labels_raw = cluster_and_plot(test_matrix, use_pca=False, title=f"layer{i}_Without PCA (Raw 64 dims)")

    #     # diff_result = compare_cluster_labels(labels_pca, labels_raw)
        
    #     co_matrix = affinity_matirx_all_layers[i]  # ç¬¬5å±‚

    #     labels = cluster_experts_per_layer(co_matrix, 4, layer_id=i, save_path=fig_dir)

    # fig_dir = f"sp_cluster_test/New/figs"
    # result_json_path = f"sp_cluster_test/New/spectral/grouping_result.json"
    # os.makedirs(fig_dir, exist_ok=True)
    
    #     # èšç±» + ä¿å­˜åˆ†ç»„ç»“æœ
    # all_groupings = spectral_clustering_all_layers(
    #     affinity_matrix_all_layers,
    #     num_groups=4,
    #     save_json_path=result_json_path
    # )

    # # é€å±‚ç»˜å›¾
    # for layer_id in range(len(affinity_matrix_all_layers)):
    #     labels = np.zeros(num_of_experts_pre_layer, dtype=int)
    #     # ä» JSON åˆ†ç»„ç»“æœè¿˜åŸ label
    #     layer_groups = all_groupings[f"layer_{layer_id}"]
    #     for group_id_str, expert_list in layer_groups.items():
    #         for expert_id in expert_list:
    #             labels[expert_id] = int(group_id_str)

    #     draw_spectral_cluster_map(
    #         affinity_matrix_all_layers[layer_id],
    #         labels,
    #         layer_id,
    #         save_dir=fig_dir
    #     )

    #     visualize_reordered_affinity(
    #         affinity_matrix_all_layers[layer_id],
    #         labels,
    #         layer_id,
    #         save_dir=fig_dir  # åŒä¸€ä¸ªæ–‡ä»¶å¤¹ä¿å­˜
    #     )
        
    #     visualize_affinity_with_clustered_ids(
    #         affinity_matrix_all_layers[layer_id],
    #         labels,
    #         layer_id,
    #         save_dir=fig_dir
    #     )
    

    co_matrix = affinity_matrix_all_layers[14]  # ç¬¬ X å±‚

    # plot_expert_affinity_graph(
    #     affinity_matrix=co_matrix,
    #     layer_id=14,
    #     save_dir="graph",
    #     threshold=1000  # æ§åˆ¶æ˜¾ç¤ºå“ªäº›è¾¹
    # )
    plot_expert_subgraph(
        affinity_matrix=affinity_matrix_all_layers[14],
        center_expert_id=4,
        layer_id=14,
        save_dir="graph",
        threshold=1000  # è¾ƒä½ä¸€ç‚¹è®©å­å›¾æœ‰è¾¹
    )